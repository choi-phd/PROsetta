---
title: "Scale linking/equating with PROsetta package"
author: "Sangdon Lim"
output:
  html_document:
    number_sections: true
    toc: true
    toc_float:
      smooth_scroll: false
    css: styles.css
vignette: >
  %\VignetteIndexEntry{Scale linking/equating with PROsetta package}
  %\VignetteEncoding{UTF-8}
  %\VignetteEngine{knitr::rmarkdown}
editor_options:
  chunk_output_type: console
---

# Introduction

This document explains how to perform scale linking/equating with the *PROsetta* package.

```{r, echo = FALSE, message = FALSE, output = 'hide'}
library(knitr)
library(kableExtra)
library(PROsetta)
```

<br>

# Load datasets

First step is to read in datasets with `loadData()`. The PROMIS Depression â€“ PHQ9 linking data, included in the package, is used for this example.

```{r cfg, results = 'hide', message = FALSE}
fp = system.file("data-raw", package = "PROsetta")
d = loadData(
  response  = "dat_DeCESD_v2.csv",
  itemmap   = "imap_DeCESD.csv",
  anchor    = "anchor_DeCESD.csv",
  input_dir = fp)
```

* `response`: Response data from current administration, containing anchor and input items. Can be a .csv filename or a data frame.
* `itemmap`: Item map specifying which items are in which instrument. Can be a .csv filename or a data frame.
* `anchor`: Item parameters for anchor items. Can be a .csv filename or a data frame.
* `input_dir`: (Optional) The path of the directory to look for the .csv files.

<br>

## Response data

The response data requires the following columns.

* `prosettaid`: The person ID of the response row. The name of this variable does not have to be `prosettaid` but should not conflict with other data (item map and anchor).
* Other columns should have the unique ID of each item as column names. The item names are referred by the `item_id` column in the anchor file, and the `item_id` column in the item map file.

Run the following code for an example.

```{r, eval = FALSE}
file.edit(system.file("data-raw", "dat_DeCESD_v2.csv", package = "PROsetta"))
```

<br>

## Item map data

The item map data requires the following columns.

* `item_id`: The unique ID of the item. The name of this column does not have to be `item_id`, but should be consistent with the anchor data. The content of this column should refer to column names in the response data.
* `instrument`: The ID of the instrument that the item belongs to (beginning from 1)
* `item_order`: The position of the item in the combined scale (beginning from 1)
* `item_name`: New names to use in the combined scale
* `ncat`: The number of response categories
* `min_score`: The minimum score of the item
* `reverse`: Whether the item has been reverse scored
* `scores`: A string containing comma-separated values for all possible scores of the item

Run the following code for an example.

```{r, eval = FALSE}
file.edit(system.file("data-raw", "imap_DeCESD.csv"  , package = "PROsetta"))
```

<br>

## Anchor data

The anchor data requires the following columns.

* `item_order`: The position of the item in the anchor scale (beginning from 1)
* `item_id`: The unique ID of the item. The name of this column does not have to be `item_id`, but should be consistent with the item map data. The content of this column should refer to column names in the response data.
* `a`: Item parameter in graded response model
* `cb1`, `cb2`, ...: Item parameters in graded response model
* `ncat`: The number of response categories

Run the following code for an example.

```{r, eval = FALSE}
file.edit(system.file("data-raw", "anchor_DeCESD.csv", package = "PROsetta"))
```

<br>

# Descriptive analysis

## Basic descriptives

The frequency table of response data is obtained by `runFrequency()`. This function is robust to cases where a response category has zero responses.

```{r freq, results = 'hide'}
runFrequency(d)
```

<br>

The frequency table can be plotted as a histogram with `plot()`. Adjust the `scale` argument to specify which scale to plot. `scale = 'combined'` represents the combined scale.

```{r, fig.align = 'center', fig.width = 7, fig.height = 7}
plot(d, scale = 'combined', title = "Combined scale")
```

```{r eval = FALSE}
plot(d, scale = 1, title = "Scale 1") # not run
plot(d, scale = 2, title = "Scale 2") # not run
```

<br>

Basic descriptive measures are obtained by `runDescriptive()`.

```{r desc, results = 'hide'}
runDescriptive(d)
```

<br>

## Classical reliability

Classical reliability measures are obtained by `runClassical()`. By default, analysis is performed only for the combined scale. Set `scalewise = TRUE` to request analysis for each scale in addition to the combined scale.

```{r alpha, cache = TRUE, eval = FALSE}
classical_table = runClassical(d, scalewise = TRUE)
classical_table$alpha$combined # alpha values for combined scale
classical_table$alpha$`1`      # alpha values for each scale, created when scalewise = TRUE
classical_table$alpha$`2`      # alpha values for each scale, created when scalewise = TRUE
```

Specifying `omega = TRUE` returns $\omega$ coefficients as well.

```{r omega, cache = TRUE, eval = FALSE}
classical_table = runClassical(d, scalewise = TRUE, omega = TRUE)
classical_table$omega$combined # omega values for combined scale
classical_table$omega$`1`      # omega values for each scale, created when scalewise = TRUE
classical_table$omega$`2`      # omega values for each scale, created when scalewise = TRUE
```

Additional arguments can be supplied to `runClassical()` to pass onto `psych::omega()`.

```{r eval = FALSE}
classical_table = runClassical(d, scalewise = TRUE, omega = TRUE, nfactors = 5) # not run
```

<br>

## Dimensionality analysis

Dimensionality analysis with CFA is performed by `runCFA()`. Set `scalewise = TRUE` to perform analysis for each scale in addition to the combined scale.

```{r cfa, cache = FALSE, results = 'hide'}
out_cfa = runCFA(d, scalewise = TRUE)
```

`runCFA()` calls for `lavaan::cfa()` internally, and can pass additional arguments onto it.
 
```{r, eval = FALSE}
out_cfa = runCFA(d, scalewise = TRUE, std.lv = TRUE) # not run
```

<br>

The analysis result for the combined scale is stored in the `combined` slot, and if `scalewise = TRUE` then the analysis for each scale is also stored in each numbered slot.

```{r}
out_cfa$combined
out_cfa$`1`
out_cfa$`2`
```

<br>

Fit indices can be obtained by using `summary()` from the *lavaan* package.

```{r}
lavaan::summary(out_cfa$combined, fit.measures = TRUE, standardized = TRUE, estimates = FALSE)
```

```{r, eval = FALSE}
lavaan::summary(out_cfa$`1`     , fit.measures = TRUE, standardized = TRUE, estimates = FALSE) # not run
lavaan::summary(out_cfa$`2`     , fit.measures = TRUE, standardized = TRUE, estimates = FALSE) # not run
```

<br>

## Item parameter calibration

`runCalibration()` performs item parameter calibration without linking to the anchor.  `runCalibration()` calls `mirt::mirt()` internally, and supplied arguments can be passed onto it.

```{r calib, cache = TRUE, results = 'hide', error = TRUE, message = FALSE}
out_calib = runCalibration(d, technical = list(NCYCLES = 1000))
```

<br>

In case of nonconvergence, `runCalibration()` explicitly raises an error and does not return its results.

```{r calib2, cache = TRUE, results = 'hide', error = TRUE, message = FALSE}
out_calib2 = runCalibration(d)
```

<br>

The output object can be summarized with functions from *mirt* package.

Use `coef()` to extract item parameters:

```{r mirt_coef, cache = TRUE}
mirt::coef(out_calib, IRTpars = TRUE, simplify = TRUE)
```

and also other commonly used functions:

```{r mirt_plot, cache = TRUE, eval = FALSE, results = 'hide'}
mirt::itemfit(out_calib, empirical.plot = 1)
mirt::itemplot(out_calib, item = 1, type = "info")
mirt::itemfit(out_calib, "S_X2", na.rm = TRUE)
```

<br>

Scale information can be plotted with `plotInfo`. The last values in arguments `scale_label`, `color`, `lty` represent the values for the combined scale.

```{r, fig.align = 'center', fig.width = 7, fig.height = 7}
plotInfo(
  out_calib, d,
  scale_label = c("PROMIS Depression", "CES-D", "Combined"),
  color = c("blue", "red", "black"),
  lty = c(1, 2, 3))
```

<br>

# Scale linking

`runLinking()` performs linking of item parameters from the response data to supplied anchor item parameters.

<br>

## Fixed parameter calibration method

Item parameter linking through fixed parameter calibration is performed by setting `method = "FIXEDPAR"`. The linked parameters are stored in the `$ipar_linked` slot.

```{r fixedpar, cache = TRUE, results = 'hide', message = FALSE}
out_link_fixedpar = runLinking(d, method = "FIXEDPAR")
```


```{r}
out_link_fixedpar$ipar_linked
```


<br>

## Linear transformation methods

Item parameter linking through linear transformation is performed by setting the `method` argument to:

* `MM` (Mean-Mean)
* `MS` (Mean-Sigma)
* `HB` (Haebara)
* `SL` (Stocking-Lord)

Arguments supplied to `runLinking` are passed onto `mirt::mirt()` internally. In case of nonconvergence in the free calibration step, `runLinking()` explicitly raises an error and does not return its results.

```{r sl, cache = TRUE, results = 'hide', error = TRUE, message = FALSE}
out_link_sl = runLinking(d, method = "SL", technical = list(NCYCLES = 1000))
out_link_sl
```

<br>

The linked parameters calibrated to the response data are stored in the `$ipar_linked` slot.

```{r}
out_link_sl$ipar_linked
```

<br>

Transformation constants for the specified method are stored in the `$constants` slot.

```{r}
out_link_sl$constants
```

<br>


## Obtaining standard scores

From the linked parameters, the raw-score to standard-score table can be obtained by `runRSSS()`.

The table contains theta values corresponding to each level of raw sum score, as well as expected scores in each scale from the theta values.

```{r}
rsss_fixedpar = runRSSS(d, out_link_fixedpar)
rsss_sl       = runRSSS(d, out_link_sl)
round(rsss_fixedpar$`2`, 3)
```

<br>

The columns represent:

* `raw_2`: observed raw score level in Scale 2
* `tscore`: EAP estimate using the supplied item parameters for Scale 2 items
* `tscore_se`: standard error of EAP estimate
* `theta`: EAP estimate using the supplied item parameters for Scale 2 items
* `theta_se`: standard error of EAP estimate
* `escore_1`: expected score in Scale 1 at the theta value
* `escore_2`: expected score in Scale 2 at the theta value
* `escore_3`: expected score in the combined scale at the theta value

<br>

## Equipercentile method: raw-raw

Equipercentile equating of observed scores is performed by `runEquateObserved()`.

Cases with missing responses are removed to be able to generate correct scores in concordance tables.

This function requires four arguments:

* `scale_from`: the index of the input scale as specified in the item map
* `scale_to`: the index of the anchor scale as specified in the item map
* `type`: set to `equipercentile` for this example
* `smooth`: the type of presmoothing to perform

By default, `runEquateObserved()` performs raw-raw equating. In this example, this maps the raw scores from Scale 2 (range 20 to 80) onto raw score equivalents in Scale 1 (range 28 to 140).

```{r eqp_raw, cache = TRUE, results = 'hide', message = FALSE}
out_equate = runEquateObserved(
  d, scale_from = 2, scale_to = 1,
  eq_type = "equipercentile", smooth = "loglinear")
```

The concordance table can be obtained from the `concordance` slot:

```{r}
out_equate$concordance
```

<br>

## Equipercentile method: raw-T-score

Direct raw-T-score equating can be triggered by specifying `type_to = 'tscore'` in `runEquateObserved()`. In this example, this maps the raw scores from Scale 2 (range 20 to 80) onto t-score equivalents in Scale 1 (mean = 50, SD = 10).

```{r eqp_dir, cache = TRUE, results = 'hide', message = FALSE}
out_equate_dir = runEquateObserved(
  d, scale_from = 2, scale_to = 1,
  type_to = "tscore", rsss = rsss_fixedpar,
  eq_type = "equipercentile", smooth = "loglinear")
```

```{r}
out_equate_dir$concordance
```

<br>

# Evaluation

The results so far produced are now compared.

<br>

## Raw scores from Scale 2

Before starting, produce raw scores from Scale 2 for each examinee.

```{r}
tmp       = getScaleSum(d, 2)
person_id = d@person_id
```

<br>

## Pattern scoring on Scale 1

To serve as reference, obtain EAP estimates of individual T-scores from Scale 1 item parameters. Here, fixed parameter calibration results are used for the item parameters.

**Note: Be careful with the `sort = F` option in `merge`, since it does not prevent rows from being sorted. It only prevents column sort.**

```{r, message = FALSE}
o1            = getTheta(d, out_link_fixedpar$ipar_linked, scale = 1)
tmp           = merge(tmp, o1$theta, by = person_id)
tmp           = tmp[, 1:3]
tmp[, 3]      = tmp[, 3] * 10 + 50
names(tmp)[3] = "fxpar_ipar1_tscore"
```

<br>

## Pattern scoring on Scale 2

First, obtain EAP estimates of individual T-scores from Scale 2 item parameters.

```{r, message = FALSE}
o2            = getTheta(d, out_link_fixedpar$ipar_linked, scale = 2)
tmp           = merge(tmp, o2$theta, by = person_id)
tmp           = tmp[, 1:4]
tmp[, 4]      = tmp[, 4] * 10 + 50
names(tmp)[4] = "fxpar_ipar2_tscore"
```

<br>

## IRT scoring for each sum score level

Second, use the RSSS table to map each raw score level of Scale 2 onto T-scores.

```{r, message = FALSE}
tmp           = merge(tmp, rsss_fixedpar$`2`, by = "raw_2")
tmp           = tmp[, 1:5]
names(tmp)[5] = "fxpar_rsss_tscore"
```

<br>

## Equipercentile method

Third, use the concordance table from direct raw-T-score equating to map each raw score level of Scale 2 onto T-scores.

```{r, message = FALSE}
tmp           = merge(tmp, out_equate_dir$concordance, by = "raw_2")
tmp           = tmp[, 1:6]
names(tmp)[6] = "eqp_tscore"
```

<br>

## Compare

Finally, use `compareScores()` to compare the obtained T-scores.

```{r}
# Reference score: IRT pattern scoring of Scale 1
compareScores(tmp$fxpar_ipar1_tscore, tmp$fxpar_ipar2_tscore) ## IRT pattern scoring of Scale 2
compareScores(tmp$fxpar_ipar1_tscore, tmp$fxpar_rsss_tscore)  ## IRT raw_2 -> tscore
compareScores(tmp$fxpar_ipar1_tscore, tmp$eqp_tscore)         ## EQP raw_2 -> tscore
```
